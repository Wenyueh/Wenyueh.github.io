
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Welcome!\nI’m Wenyue Hua, senior researcher at Microsoft Research, AI Frontiers. I was a postdoctoral researcher at University of California, Santa Barbara, working with Prof. William Yang Wang (2024 - 2025). I obtained my Ph.D. degree from Rutgers University, New Brunswick (2020 - 2024). I’m honored to be advised by Prof. Yongfeng Zhang. I received MA in Linguistics at Rutgers in 2020 (proudly advised by Prof. Adam Jardine) and BA in Linguistics and Philosophy and BS in Mathematics at UCLA in 2018 (proudly advised by Prof. Edward Keenan).\nMy research interests lie in Large Language Models and its various application, such as LLM-based agent, multi-agent system, generative recommender system, LLM reasoning. I care about the decision-making ability, safety, and efficiency of LLM-based agents.\nCollaboration \u0026amp; Mentoring I welcome discussions about AI agents and am open to collaborations with researchers and industry professionals. I also enjoy mentoring students at various stages of their academic journey.\nFeel free to schedule a conversation here to explore potential partnerships or discuss recent developments in the field.\n","date":1756771200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1756771200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Welcome!\nI’m Wenyue Hua, senior researcher at Microsoft Research, AI Frontiers. I was a postdoctoral researcher at University of California, Santa Barbara, working with Prof. William Yang Wang (2024 - 2025).","tags":null,"title":"Wenyue Hua","type":"authors"},{"authors":null,"categories":null,"content":"","date":1758844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1758844800,"objectID":"de27b7d8a412cd22b3018c992426dee4","permalink":"https://Wenyueh.github.io/en/talk/Gave-an-invited-talk-about-Magentic-Marketplace-at-Recsys-2025-EARL-workshop/","publishdate":"2025-09-26T00:00:00Z","relpermalink":"/en/talk/Gave-an-invited-talk-about-Magentic-Marketplace-at-Recsys-2025-EARL-workshop/","section":"event","summary":"","tags":null,"title":"Gave an invited talk about Magentic Marketplace at Recsys 2025 EARL workshop!","type":"event"},{"authors":null,"categories":null,"content":"","date":1757462400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1757462400,"objectID":"2e7688df2ab65be582c4207325090f3a","permalink":"https://Wenyueh.github.io/en/talk/Gave-a-5-minute-demo-at-Made-in-Microsoft-NYC/","publishdate":"2025-09-10T00:00:00Z","relpermalink":"/en/talk/Gave-a-5-minute-demo-at-Made-in-Microsoft-NYC/","section":"event","summary":"","tags":null,"title":"Gave a 5-minute demo at Made in Microsoft NYC!","type":"event"},{"authors":["Yilin Guan","Qingfeng Lan","Fei Sun","Dujian Ding","Devang Acharya","Chi Wang","William Yang Wang","Wenyue Hua"],"categories":null,"content":" Abstract Despite their remarkable success in complex tasks propelling widespread adoption, large language-model-based agents still face critical deployment challenges due to prohibitive latency and inference costs. While recent work has explored various methods to accelerate inference, existing approaches suffer from significant limitations – they either fail to preserve performance fidelity, require extensive offline training of router modules, or incur excessive operational costs. Moreover, they provide minimal user control over the tradeoff between acceleration and other performance metrics. To address these gaps, we introduce Dynamic Speculative Planning (DSP), an asynchronous online reinforcement learning framework that provides lossless acceleration with substantially reduced costs without requiring additional pre-deployment preparation. DSP explicitly optimizes a joint objective balancing end-to-end latency against dollar cost, allowing practitioners to adjust a single parameter that steers the system toward faster responses, cheaper operation, or any point along this continuum. Experiments on two standard agent benchmarks demonstrate that DSP achieves comparable efficiency to the fastest lossless acceleration method while reducing total cost by 30% and unnecessary cost up to 60%.\n","date":1756771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1756771200,"objectID":"9fc00f5bacd8f99ed14d09d463495fec","permalink":"https://Wenyueh.github.io/en/publication/dynamicspec/","publishdate":"2025-09-02T00:00:00Z","relpermalink":"/en/publication/dynamicspec/","section":"publication","summary":"Despite their remarkable success in complex tasks propelling widespread adoption, large language-model-based agents still face critical deployment challenges due to prohibitive latency and inference costs. While recent work has explored various methods to accelerate inference, existing approaches suffer from significant limitations -- they either fail to preserve performance fidelity, require extensive offline training of router modules, or incur excessive operational costs. Moreover, they provide minimal user control over the tradeoff between acceleration and other performance metrics. To address these gaps, we introduce Dynamic Speculative Planning (DSP), an asynchronous online reinforcement learning framework that provides lossless acceleration with substantially reduced costs without requiring additional pre-deployment preparation. DSP explicitly optimizes a joint objective balancing end-to-end latency against dollar cost, allowing practitioners to adjust a single parameter that steers the system toward faster responses, cheaper operation, or any point along this continuum. Experiments on two standard agent benchmarks demonstrate that DSP achieves comparable efficiency to the fastest lossless acceleration method while reducing total cost by 30% and unnecessary cost up to 60%.","tags":[],"title":"Dynamic Speculative Agent Planning","type":"publication"},{"authors":["Wenyue Hua","Dujian Ding","Yile Gu","Yujie Ren","Kai Mei","Minghua Ma","William Yang Wang"],"categories":null,"content":" Abstract Conventional operating system scheduling algorithms are largely content-ignorant, making decisions based on factors such as latency or fairness without considering the actual intents or semantics of processes. Consequently, these algorithms often do not prioritize tasks that require urgent attention or carry higher importance, such as in emergency management scenarios. However, recent advances in language models enable semantic analysis of processes, allowing for more intelligent and context-aware scheduling decisions. In this paper, we introduce the concept of semantic scheduling in scheduling of requests from large language models (LLM), where the semantics of the process guide the scheduling priorities. We present a novel scheduling algorithm with optimal time complexity, designed to minimize the overall waiting time in LLM-based prompt scheduling. To illustrate its effectiveness, we present a medical emergency management application, underscoring the potential benefits of semantic scheduling for critical, time-sensitive tasks.\n","date":1749772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1749772800,"objectID":"8139e237a6f30b297b5671c91771c4db","permalink":"https://Wenyueh.github.io/en/publication/semanticscheduling/","publishdate":"2025-06-13T00:00:00Z","relpermalink":"/en/publication/semanticscheduling/","section":"publication","summary":"Conventional operating system scheduling algorithms are largely content-ignorant, making decisions based on factors such as latency or fairness without considering the actual intents or semantics of processes. Consequently, these algorithms often do not prioritize tasks that require urgent attention or carry higher importance, such as in emergency management scenarios. However, recent advances in language models enable semantic analysis of processes, allowing for more intelligent and context-aware scheduling decisions. In this paper, we introduce the concept of semantic scheduling in scheduling of requests from large language models (LLM), where the semantics of the process guide the scheduling priorities. We present a novel scheduling algorithm with optimal time complexity, designed to minimize the overall waiting time in LLM-based prompt scheduling. To illustrate its effectiveness, we present a medical emergency management application, underscoring the potential benefits of semantic scheduling for critical, time-sensitive tasks.","tags":[],"title":"Semantic Scheduling for LLM Inference","type":"publication"},{"authors":["Wenyue Hua"],"categories":null,"content":" Abstract Machine Learning Systems (MLSys) is an emerging field that sits at the intersection of systems engineering and machine learning, focusing on the practical challenges of deploying, scaling, and optimizing AI systems in real-world environments. MLSys addresses the significant obstacles in designing and implementing systems that support ML models in production, recognizing the radically different development and deployment profiles of modern ML methods compared to traditional software systems. The field encompasses hardware systems for ML, software systems for ML, and optimizations that go beyond predictive accuracy to consider factors like latency, throughput, energy efficiency, and cost-effectiveness.\n","date":1749686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1749686400,"objectID":"73fa25f2c2d3237af40dd14cda783fd5","permalink":"https://Wenyueh.github.io/en/project/MLSys/","publishdate":"2025-06-12T00:00:00Z","relpermalink":"/en/project/MLSys/","section":"project","summary":"Abstract Machine Learning Systems (MLSys) is an emerging field that sits at the intersection of systems engineering and machine learning, focusing on the practical challenges of deploying, scaling, and optimizing AI systems in real-world environments.","tags":[],"title":"Machine Learning System","type":"project"},{"authors":null,"categories":null,"content":"","date":1747267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1747267200,"objectID":"3b3dda90a12dfe50e6ed6e642e189ea3","permalink":"https://Wenyueh.github.io/en/talk/8-papers-accepted-to-ACL-2025-Congrats-to-all-my-coauthors/","publishdate":"2025-05-15T00:00:00Z","relpermalink":"/en/talk/8-papers-accepted-to-ACL-2025-Congrats-to-all-my-coauthors/","section":"event","summary":"","tags":null,"title":"8 papers accepted to ACL 2025! Congrats to all my coauthors!","type":"event"},{"authors":null,"categories":null,"content":"","date":1747008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1747008000,"objectID":"a1e035781f81d6fe351bd64d58ea920a","permalink":"https://Wenyueh.github.io/en/talk/Gave-a-talk-at-FLaNN-Seminars-on-InductionBench./","publishdate":"2025-05-12T00:00:00Z","relpermalink":"/en/talk/Gave-a-talk-at-FLaNN-Seminars-on-InductionBench./","section":"event","summary":"","tags":null,"title":"Gave a talk at FLaNN Seminars on InductionBench.","type":"event"},{"authors":null,"categories":null,"content":"","date":1744588800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1744588800,"objectID":"e9e9e56226267e76380c04844e51f756","permalink":"https://Wenyueh.github.io/en/talk/Gave-a-talk-at-Ploutos-on-InductionBench./","publishdate":"2025-04-14T00:00:00Z","relpermalink":"/en/talk/Gave-a-talk-at-Ploutos-on-InductionBench./","section":"event","summary":"","tags":null,"title":"Gave a talk at Ploutos on InductionBench.","type":"event"},{"authors":null,"categories":null,"content":"","date":1743379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743379200,"objectID":"049880cef8a4dec01527b4611aa07413","permalink":"https://Wenyueh.github.io/en/talk/Gave-a-talk-at-Northeastern-University-on-Agent-building-with-respect-to-safety-and-efficiency./","publishdate":"2025-03-31T00:00:00Z","relpermalink":"/en/talk/Gave-a-talk-at-Northeastern-University-on-Agent-building-with-respect-to-safety-and-efficiency./","section":"event","summary":"","tags":null,"title":"Gave a talk at Northeastern University on Agent building with respect to safety and efficiency.","type":"event"},{"authors":null,"categories":null,"content":"","date":1740700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740700800,"objectID":"51678f39792f305fa6882a95f5d0eead","permalink":"https://Wenyueh.github.io/en/talk/Gave-a-talk-at-Amazon-on-LLM-based-Agent-on-recommendation-system./","publishdate":"2025-02-28T00:00:00Z","relpermalink":"/en/talk/Gave-a-talk-at-Amazon-on-LLM-based-Agent-on-recommendation-system./","section":"event","summary":"","tags":null,"title":"Gave a talk at Amazon on LLM-based Agent on recommendation system.","type":"event"},{"authors":["Wenyue Hua","Tyler Wong","Fei Sun","Liangming Pan","Adam Jardine","William Yang Wang"],"categories":null,"content":" Abstract Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs’ inductive reasoning capabilities. Coda and data are available this url.\n","date":1740009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740009600,"objectID":"41045992e4b1d45267359bcbdbde0f20","permalink":"https://Wenyueh.github.io/en/publication/inductionbench/","publishdate":"2025-02-20T00:00:00Z","relpermalink":"/en/publication/inductionbench/","section":"publication","summary":"Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities. Coda and data are available [this url](https://github.com/Wenyueh/inductive_reasoning_benchmark).","tags":[],"title":"InductionBench: LLMs Fail in the Simplest Complexity Class","type":"publication"},{"authors":["Chaoran Chen","Bingsheng Yao","Ruishi Zou","Wenyue Hua","Weimin Lyu","Yanfang Ye","Toby Jia-Jun Li","Dakuo Wang"],"categories":null,"content":" Abstract Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature. Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.\n","date":1739836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1739836800,"objectID":"a733762d81b12b3220d31b06de534b5a","permalink":"https://Wenyueh.github.io/en/publication/RPAEval/","publishdate":"2025-02-18T00:00:00Z","relpermalink":"/en/publication/RPAEval/","section":"publication","summary":"Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature. Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.","tags":[],"title":"Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents","type":"publication"},{"authors":null,"categories":null,"content":"","date":1739404800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1739404800,"objectID":"565aef212bb7d20523aa909c0724c4c1","permalink":"https://Wenyueh.github.io/en/talk/Gave-a-talk-at-AG2-on-Interactive-Speculative-Planning./","publishdate":"2025-02-13T00:00:00Z","relpermalink":"/en/talk/Gave-a-talk-at-AG2-on-Interactive-Speculative-Planning./","section":"event","summary":"","tags":null,"title":"Gave a talk at AG2 on Interactive Speculative Planning.","type":"event"},{"authors":null,"categories":null,"content":"","date":1738886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738886400,"objectID":"f2c41198fb11a74c97039bd080d96689","permalink":"https://Wenyueh.github.io/en/talk/Gave-a-job-talk-at-Microsoft-Research-on-Agent-building-with-respect-to-safety-and-efficiency./","publishdate":"2025-02-07T00:00:00Z","relpermalink":"/en/talk/Gave-a-job-talk-at-Microsoft-Research-on-Agent-building-with-respect-to-safety-and-efficiency./","section":"event","summary":"","tags":null,"title":"Gave a job talk at Microsoft Research on Agent building with respect to safety and efficiency.","type":"event"},{"authors":null,"categories":null,"content":"","date":1738108800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738108800,"objectID":"5560e884fa4e301e4503fa5b98647c16","permalink":"https://Wenyueh.github.io/en/talk/Selected-as-KAUST-AI-Rising-Star-in-2025/","publishdate":"2025-01-29T00:00:00Z","relpermalink":"/en/talk/Selected-as-KAUST-AI-Rising-Star-in-2025/","section":"event","summary":"","tags":null,"title":"Selected as KAUST AI Rising Star in 2025!","type":"event"},{"authors":null,"categories":null,"content":"","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"296e29e3c1144a311106ceb5c7e25e26","permalink":"https://Wenyueh.github.io/en/talk/2-papers-accepted-to-NAACL-2025-and-2-papers-accepted-to-ICLR-2025/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/en/talk/2-papers-accepted-to-NAACL-2025-and-2-papers-accepted-to-ICLR-2025/","section":"event","summary":"","tags":null,"title":"2 papers accepted to NAACL 2025 and 2 papers accepted to ICLR 2025!","type":"event"},{"authors":null,"categories":null,"content":"","date":1736899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1736899200,"objectID":"b1152369674fe107f7c74d3106289b5c","permalink":"https://Wenyueh.github.io/en/talk/Gave-a-talk-at-Vanderbilt-University-on-Game-theoretic-LLM./","publishdate":"2025-01-15T00:00:00Z","relpermalink":"/en/talk/Gave-a-talk-at-Vanderbilt-University-on-Game-theoretic-LLM./","section":"event","summary":"","tags":null,"title":"Gave a talk at Vanderbilt University on Game-theoretic LLM.","type":"event"},{"authors":null,"categories":null,"content":"","date":1734739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734739200,"objectID":"1aefb5784db215c3cc63d2006b56f2ee","permalink":"https://Wenyueh.github.io/en/talk/Gave-a-talk-at-Swarma-Organization-on-Game-theoretic-LLM./","publishdate":"2024-12-21T00:00:00Z","relpermalink":"/en/talk/Gave-a-talk-at-Swarma-Organization-on-Game-theoretic-LLM./","section":"event","summary":"","tags":null,"title":"Gave a talk at Swarma Organization on Game-theoretic LLM.","type":"event"},{"authors":["Ruiwen Zhou","Wenyue Hua","Liangming Pan","Sitao Cheng","Xiaobao Wu","En Yu","William Yang Wang"],"categories":null,"content":" Abstract This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains – airline baggage fees, NBA transactions, and tax regulations – RuleArena assesses LLMs’ proficiency in handling intricate natural language instructions that demand long-context understanding, logical reasoning, and accurate mathematical computation. Two key attributes distinguish RuleArena from traditional rule-based reasoning benchmarks: (1) it extends beyond standard first-order logic representations, and (2) it is grounded in authentic, practical scenarios, providing insights into the suitability and reliability of LLMs for real-world applications. Our findings reveal several notable limitations in LLMs: (1) they struggle to identify and apply the appropriate rules, frequently becoming confused by similar but distinct regulations, (2) they cannot consistently perform accurate mathematical computations, even when they correctly identify the relevant rules, and (3) in general, they perform poorly in the benchmark. These results highlight significant challenges in advancing LLMs’ rule-guided reasoning capabilities in real-life applications.\n","date":1733961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733961600,"objectID":"e7ec6be3a3875bdce6e0057d9eb11201","permalink":"https://Wenyueh.github.io/en/publication/RuleArena/","publishdate":"2024-12-12T00:00:00Z","relpermalink":"/en/publication/RuleArena/","section":"publication","summary":"This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains -- airline baggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs' proficiency in handling intricate natural language instructions that demand long-context understanding, logical reasoning, and accurate mathematical computation. Two key attributes distinguish RuleArena from traditional rule-based reasoning benchmarks -- (1) it extends beyond standard first-order logic representations, and (2) it is grounded in authentic, practical scenarios, providing insights into the suitability and reliability of LLMs for real-world applications. Our findings reveal several notable limitations in LLMs -- (1) they struggle to identify and apply the appropriate rules, frequently becoming confused by similar but distinct regulations, (2) they cannot consistently perform accurate mathematical computations, even when they correctly identify the relevant rules, and (3) in general, they perform poorly in the benchmark. These results highlight significant challenges in advancing LLMs' rule-guided reasoning capabilities in real-life applications.","tags":[],"title":"Rulearena: A benchmark for rule-guided reasoning with llms in real-world scenarios","type":"publication"},{"authors":null,"categories":null,"content":"","date":1732665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732665600,"objectID":"1554932d68d586d6eb30b7d86fc6863c","permalink":"https://Wenyueh.github.io/en/talk/Gave-a-talk-at-BAAI-Hub-on-Game-theoretic-LLM./","publishdate":"2024-11-27T00:00:00Z","relpermalink":"/en/talk/Gave-a-talk-at-BAAI-Hub-on-Game-theoretic-LLM./","section":"event","summary":"","tags":null,"title":"Gave a talk at BAAI Hub on Game-theoretic LLM.","type":"event"},{"authors":["Wenyue Hua","Ollie Liu","Lingyao Li","Alfonso Amayuelas","Julie Chen","Lucas Jiang","Lizhou Fan","Fei Sun","William Yang Wang","Xintong Wang","Yongfeng Zhang"],"categories":null,"content":" Abstract This paper investigates the rationality of large language models (LLMs) in strategic decision-making contexts, specifically within the framework of game theory. We evaluate several state-of-the-art LLMs across a spectrum of complete-information and incomplete-information games. Our findings reveal that LLMs frequently deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees. To address these limitations, we design multiple game-theoretic workflows that guide the reasoning and decision-making processes of LLMs. These workflows aim to enhance the models’ ability to compute Nash Equilibria and make rational choices, even under conditions of uncertainty and incomplete information. Experimental results demonstrate that the adoption of these workflows significantly improves the rationality and robustness of LLMs in game-theoretic tasks. Specifically, with the workflow, LLMs exhibit marked improvements in identifying optimal strategies, achieving near-optimal allocations in negotiation scenarios, and reducing susceptibility to exploitation during negotiations. Furthermore, we explore the meta-strategic considerations of whether it is rational for agents to adopt such workflows, recognizing that the decision to use or forgo the workflow constitutes a game-theoretic issue in itself. Our research contributes to a deeper understanding of LLMs’ decision-making capabilities in strategic contexts and provides insights into enhancing their rationality through structured workflows. The findings have implications for the development of more robust and strategically sound AI agents capable of navigating complex interactive environments. Code and data supporting this study are available at this url.\n","date":1731024000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1731024000,"objectID":"39e5a3fcec5c4a5169d9b91b294e0a45","permalink":"https://Wenyueh.github.io/en/publication/gametheory/","publishdate":"2024-11-08T00:00:00Z","relpermalink":"/en/publication/gametheory/","section":"publication","summary":"This paper investigates the rationality of large language models (LLMs) in strategic decision-making contexts, specifically within the framework of game theory. We evaluate several state-of-the-art LLMs across a spectrum of complete-information and incomplete-information games. Our findings reveal that LLMs frequently deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees. To address these limitations, we design multiple game-theoretic workflows that guide the reasoning and decision-making processes of LLMs. These workflows aim to enhance the models' ability to compute Nash Equilibria and make rational choices, even under conditions of uncertainty and incomplete information. Experimental results demonstrate that the adoption of these workflows significantly improves the rationality and robustness of LLMs in game-theoretic tasks. Specifically, with the workflow, LLMs exhibit marked improvements in identifying optimal strategies, achieving near-optimal allocations in negotiation scenarios, and reducing susceptibility to exploitation during negotiations. Furthermore, we explore the meta-strategic considerations of whether it is rational for agents to adopt such workflows, recognizing that the decision to use or forgo the workflow constitutes a game-theoretic issue in itself. Our research contributes to a deeper understanding of LLMs' decision-making capabilities in strategic contexts and provides insights into enhancing their rationality through structured workflows. The findings have implications for the development of more robust and strategically sound AI agents capable of navigating complex interactive environments.Coda and data are available [this url](https://github.com/Wenyueh/game_theory).","tags":[],"title":"Game-theoretic LLM: Agent Workflow for Negotiation Games","type":"publication"},{"authors":null,"categories":null,"content":"","date":1730937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730937600,"objectID":"98979bfbca5abaea14dea8046d2195ff","permalink":"https://Wenyueh.github.io/en/talk/Gave-a-talk-at-BIMSA-Beijing-Institute-of-Mathematical-Sciences-and-Applications-on-LLM-based-Agent-on-decision-making-in-international-relationships-and-game-theory./","publishdate":"2024-11-07T00:00:00Z","relpermalink":"/en/talk/Gave-a-talk-at-BIMSA-Beijing-Institute-of-Mathematical-Sciences-and-Applications-on-LLM-based-Agent-on-decision-making-in-international-relationships-and-game-theory./","section":"event","summary":"","tags":null,"title":"Gave a talk at BIMSA (Beijing Institute of Mathematical Sciences and Applications) on LLM-based Agent on decision making in international relationships and game theory.","type":"event"},{"authors":["Wenyue Hua","Mengting Wan","Shashank Vadrevu","Ryan Nadel","Yongfeng Zhang","Chi Wang"],"categories":null,"content":" Abstract Agents, as user-centric tools, are increasingly deployed for human task delegation, assisting with a broad spectrum of requests by generating thoughts, engaging with user proxies, and producing action plans. However, agents based on large language models (LLMs) often face substantial planning latency due to two primary factors – the efficiency limitations of the underlying LLMs due to their large size and high demand, and the structural complexity of the agents due to the extensive generation of intermediate thoughts to produce the final output. Given that inefficiency in service provision can undermine the value of automation for users, this paper presents a human-centered efficient agent planning method – Interactive Speculative Planning – aiming at enhancing the efficiency of agent planning through both system design and human-AI interaction. Our approach advocates for the co-design of the agent system and user interface, underscoring the importance of an agent system that can fluidly manage user interactions and interruptions. By integrating human interruptions as a fundamental component of the system, we not only make it more user-centric but also expedite the entire process by leveraging human-in-the-loop interactions to provide accurate intermediate steps. Code and data will be released.\n","date":1727654400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1727654400,"objectID":"acb806b9007cee2f5f8f6aa4a6b8e418","permalink":"https://Wenyueh.github.io/en/publication/interactive_sp/","publishdate":"2024-09-30T00:00:00Z","relpermalink":"/en/publication/interactive_sp/","section":"publication","summary":"Agents, as user-centric tools, are increasingly deployed for human task delegation, assisting with a broad spectrum of requests by generating thoughts, engaging with user proxies, and producing action plans. However, agents based on large language models (LLMs) often face substantial planning latency due to two primary factors -- the efficiency limitations of the underlying LLMs due to their large size and high demand, and the structural complexity of the agents due to the extensive generation of intermediate thoughts to produce the final output. Given that inefficiency in service provision can undermine the value of automation for users, this paper presents a human-centered efficient agent planning method -- Interactive Speculative Planning -- aiming at enhancing the efficiency of agent planning through both system design and human-AI interaction. Our approach advocates for the co-design of the agent system and user interface, underscoring the importance of an agent system that can fluidly manage user interactions and interruptions. By integrating human interruptions as a fundamental component of the system, we not only make it more user-centric but also expedite the entire process by leveraging human-in-the-loop interactions to provide accurate intermediate steps. Code and data will be released.","tags":[],"title":"Interactive Speculative Planning: Enhance Agent Efficiency through Co-design of System and User Interface","type":"publication"},{"authors":["Wenyue Hua","Kaijie Zhu","Lingyao Li","Lizhou Fan","Shuhang Lin","Mingyu Jin","Haochen Xue","Zelong Li","Jindong Wang","Yongfeng Zhang"],"categories":null,"content":" Abstract This study intends to systematically disentangle pure logic reasoning and text understanding by investigating the contrast across abstract and contextualized logical problems from a comprehensive set of domains. We explore whether LLMs demonstrate genuine reasoning capabilities across various domains when the underlying logical structure remains constant. We focus on two main questions (1) Can abstract logical problems alone accurately benchmark an LLM’s reasoning ability in real-world scenarios, disentangled from contextual support in practical settings? (2) Does fine-tuning LLMs on abstract logic problem generalize to contextualized logic problems and vice versa? To investigate these questions, we focus on standard propositional logic, specifically propositional deductive and abductive logic reasoning. In particular, we construct instantiated datasets for deductive and abductive reasoning with 4 levels of difficulty, encompassing 12 distinct categories or domains based on the categorization of Wikipedia. Our experiments aim to provide insights into disentangling context in logical reasoning and the true reasoning capabilities of LLMs and their generalization potential.\n","date":1717286400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717286400,"objectID":"520427c29c6d0087bd4facffbc9d5b9c","permalink":"https://Wenyueh.github.io/en/publication/disentangling_logic/","publishdate":"2024-06-02T00:00:00Z","relpermalink":"/en/publication/disentangling_logic/","section":"publication","summary":"This study intends to systematically disentangle pure logic reasoning and text understanding by investigating the contrast across abstract and contextualized logical problems from a comprehensive set of domains. We explore whether LLMs demonstrate genuine reasoning capabilities across various domains when the underlying logical structure remains constant. We focus on two main questions (1) Can abstract logical problems alone accurately benchmark an LLM's reasoning ability in real-world scenarios, disentangled from contextual support in practical settings? (2) Does fine-tuning LLMs on abstract logic problem generalize to contextualized logic problems and vice versa? To investigate these questions, we focus on standard propositional logic, specifically propositional deductive and abductive logic reasoning. In particular, we construct instantiated datasets for deductive and abductive reasoning with 4 levels of difficulty, encompassing 12 distinct categories or domains based on the categorization of Wikipedia. Our experiments aim to provide insights into disentangling context in logical reasoning and the true reasoning capabilities of LLMs and their generalization potential.","tags":[],"title":"Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities","type":"publication"},{"authors":["Shuhang Lin","Wenyue Hua","Lingyao Li","Che-Jui Chang","Lizhou Fan","Jianchao Ji","Hang Hua","Mingyu Jin","Jiebo Luo","Yongfeng Zhang"],"categories":null,"content":" Abstract This paper presents BattleAgent, an emulation system that combines the Large Vision-Language Model and Multi-agent System. This novel system aims to simulate complex dynamic interactions among multiple agents, as well as between agents and their environments, over a period of time. It emulates both the decision-making processes of leaders and the viewpoints of ordinary participants, such as soldiers. The emulation showcases the current capabilities of agents, featuring fine-grained multi-modal interactions between agents and landscapes. It develops customizable agent structures to meet specific situational requirements, for example, a variety of battle-related activities like scouting and trench digging. These components collaborate to recreate historical events in a lively and comprehensive manner while offering insights into the thoughts and feelings of individuals from diverse viewpoints. The technological foundations of BattleAgent establish detailed and immersive settings for historical battles, enabling individual agents to partake in, observe, and dynamically respond to evolving battle scenarios. This methodology holds the potential to substantially deepen our understanding of historical events, particularly through individual accounts. Such initiatives can also aid historical research, as conventional historical narratives often lack documentation and prioritize the perspectives of decision-makers, thereby overlooking the experiences of ordinary individuals. BattelAgent illustrates AI’s potential to revitalize the human aspect in crucial social events, thereby fostering a more nuanced collective understanding and driving the progressive development of human society.\n","date":1713139200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713139200,"objectID":"ae5f7e31b9c6f33e183fe657a887490f","permalink":"https://Wenyueh.github.io/en/publication/BattleAgent/","publishdate":"2024-04-15T00:00:00Z","relpermalink":"/en/publication/BattleAgent/","section":"publication","summary":"This paper presents BattleAgent, an emulation system that combines the Large Vision-Language Model and Multi-agent System. This novel system aims to simulate complex dynamic interactions among multiple agents, as well as between agents and their environments, over a period of time. It emulates both the decision-making processes of leaders and the viewpoints of ordinary participants, such as soldiers. The emulation showcases the current capabilities of agents, featuring fine-grained multi-modal interactions between agents and landscapes. It develops customizable agent structures to meet specific situational requirements, for example, a variety of battle-related activities like scouting and trench digging. These components collaborate to recreate historical events in a lively and comprehensive manner while offering insights into the thoughts and feelings of individuals from diverse viewpoints. The technological foundations of BattleAgent establish detailed and immersive settings for historical battles, enabling individual agents to partake in, observe, and dynamically respond to evolving battle scenarios. This methodology holds the potential to substantially deepen our understanding of historical events, particularly through individual accounts. Such initiatives can also aid historical research, as conventional historical narratives often lack documentation and prioritize the perspectives of decision-makers, thereby overlooking the experiences of ordinary individuals. BattelAgent illustrates AI's potential to revitalize the human aspect in crucial social events, thereby fostering a more nuanced collective understanding and driving the progressive development of human society.","tags":[],"title":"BattleAgent: Multi-modal Dynamic Emulation on Historical Battles to Complement Historical Analysis","type":"publication"},{"authors":["Kai Mei","Xi Zhu","Wujiang Xu","Wenyue Hua","Mingyu Jin","Zelong Li","Shuyuan Xu","Ruosong Ye","Yingqiang Ge","Yongfeng Zhang"],"categories":null,"content":" Abstract LLM-based intelligent agents face significant deployment challenges, particularly related to resource management. Allowing unrestricted access to LLM or tool resources can lead to inefficient or even potentially harmful resource allocation and utilization for agents. Furthermore, the absence of proper scheduling and resource management mechanisms in current agent designs hinders concurrent processing and limits overall system efficiency. As the diversity and complexity of agents continue to grow, addressing these resource management issues becomes increasingly critical to LLM-based agent systems. To address these challenges, this paper proposes the architecture of AIOS (LLM-based AI Agent Operating System) under the context of managing LLM-based agents. It introduces a novel architecture for serving LLM-based agents by isolating resources and LLM-specific services from agent applications into an AIOS kernel. This AIOS kernel provides fundamental services (e.g., scheduling, context management, memory management, storage management, access control) and efficient management of resources (e.g., LLM and external tools) for runtime agents. To enhance usability, AIOS also includes an AIOS-Agent SDK, a comprehensive suite of APIs designed for utilizing functionalities provided by the AIOS kernel. Experimental results demonstrate that using AIOS can achieve up to 2.1x faster execution for serving agents built by various agent frameworks. The source code is available at this url.\n","date":1711324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711324800,"objectID":"32a3aa2e4dc17973243195301855e6e8","permalink":"https://Wenyueh.github.io/en/publication/aios/","publishdate":"2024-03-25T00:00:00Z","relpermalink":"/en/publication/aios/","section":"publication","summary":"LLM-based intelligent agents face significant deployment challenges, particularly related to resource management. Allowing unrestricted access to LLM or tool resources can lead to inefficient or even potentially harmful resource allocation and utilization for agents. Furthermore, the absence of proper scheduling and resource management mechanisms in current agent designs hinders concurrent processing and limits overall system efficiency. As the diversity and complexity of agents continue to grow, addressing these resource management issues becomes increasingly critical to LLM-based agent systems. To address these challenges, this paper proposes the architecture of AIOS (LLM-based AI Agent Operating System) under the context of managing LLM-based agents. It introduces a novel architecture for serving LLM-based agents by isolating resources and LLM-specific services from agent applications into an AIOS kernel. This AIOS kernel provides fundamental services (e.g., scheduling, context management, memory management, storage management, access control) and efficient management of resources (e.g., LLM and external tools) for runtime agents. To enhance usability, AIOS also includes an AIOS-Agent SDK, a comprehensive suite of APIs designed for utilizing functionalities provided by the AIOS kernel. Experimental results demonstrate that using AIOS can achieve up to 2.1x faster execution for serving agents built by various agent frameworks. The source code is available at [this url](https://github.com/agiresearch/AIOS).","tags":[],"title":"AIOS: LLM Agent Operating System","type":"publication"},{"authors":["Guo Lin","Wenyue Hua","Zhengting Wang","Mingyu Jin","Lizhou Fan","Yongfeng Zhang"],"categories":null,"content":" Abstract Cloud-based Large Language Models such as ChatGPT have become increasingly integral to daily operations. Nevertheless, they also introduce privacy concerns: firstly, numerous studies underscore the risks to user privacy posed by jailbreaking cloud-based LLMs; secondly, the LLM service providers have access to all user data, which deters individuals from confidently utilizing such services. To address such concerns, we propose a simple yet effective paradigm, EmojiPrompt, to protect user privacy. At its core, EmojiPrompt performs generative transformation, obfuscating private data within prompts with linguistic and non-linguistic elements before submitting them to cloud-based LLMs. We evaluate EmojiPrompt’s performance across 8 datasets from various domains. We also propose simulated inference attacks to assess EmojiPrompt’s ability to preserve user privacy. The results demonstrate that EmojiPrompt effectively obfuscates user private data, while largely maintaining, or even enhancing, performances compared to the unobfuscated version. Furthermore, EmojiPrompt’s atomic-level obfuscation allows it to function exclusively with cloud-based LLMs. For source code, please refer to this url.\n","date":1707350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707350400,"objectID":"adb971a9e078e69ebc5089fc30f157c0","permalink":"https://Wenyueh.github.io/en/publication/emoji/","publishdate":"2024-02-08T00:00:00Z","relpermalink":"/en/publication/emoji/","section":"publication","summary":"Cloud-based Large Language Models (LLMs) such as ChatGPT have become increasingly integral to daily operations. Nevertheless, they also introduce privacy concerns -- firstly, numerous studies underscore the risks to user privacy posed by jailbreaking cloud-based LLMs; secondly, the LLM service providers have access to all user data, which deters individuals from confidently utilizing such services. To address such concerns, we propose a simple yet effective paradigm, EmojiPrompt, to protect user privacy. At its core, EmojiPrompt performs generative transformation, obfuscating private data within prompts with linguistic and non-linguistic elements before submitting them to cloud-based LLMs. We evaluate EmojiPrompt’s performance across 8 datasets from various domains. We also propose simulated inference attacks to assess EmojiPrompt’s ability to preserve user privacy. The results demonstrate that EmojiPrompt effectively obfuscates user private data, while largely maintaining, or even enhancing, performances compared to the unobfuscated version. Furthermore, EmojiPrompt’s atomic-level obfuscation allows it to function exclusively with cloud-based LLMs. For source code, please refer to [this url](https://github.com/agiresearch/EmojiCrypt).","tags":[],"title":"EmojiPrompt: Generative Prompt Obfuscation for Privacy-Preserving Communication with Cloud-based LLMs","type":"publication"},{"authors":["Wenyue Hua","Xianjun Yang","Zelong Li","Wei Cheng","Yongfeng Zhang"],"categories":null,"content":" Abstract Recent advancements in Large Language Models (LLMs) have shown remarkable capabilities in reasoning, prompting a surge in research aimed at developing trustworthy LLMs. The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment in everyday human activities, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies – pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent’s safety by identifying and preventing potential challenges. Furthermore, we explore the intricate relationships between safety and helpfulness, and model’s reasoning ability and its efficacy as a safe agent. We argue that a robust reasoning ability is a fundamental prerequisite for an LLM to function safely as an agent. This paper underscores the imperative of integrating safety awareness and trustworthiness into the design and deployment of LLM-based agents, not only to enhance their performance but also to ensure their responsible integration into human-centric environments.\n","date":1705708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705708800,"objectID":"8c28ff92379633d41ae2165f696a8a0b","permalink":"https://Wenyueh.github.io/en/publication/trustagent/","publishdate":"2024-01-20T00:00:00Z","relpermalink":"/en/publication/trustagent/","section":"publication","summary":"Recent advancements in Large Language Models (LLMs) have shown remarkable capabilities in reasoning, prompting a surge in research aimed at developing trustworthy LLMs. The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment in everyday human activities, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies -- pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential challenges. Furthermore, we explore the intricate relationships between safety and helpfulness, and model's reasoning ability and its efficacy as a safe agent. We argue that a robust reasoning ability is a fundamental prerequisite for an LLM to function safely as an agent. This paper underscores the imperative of integrating safety awareness and trustworthiness into the design and deployment of LLM-based agents, not only to enhance their performance but also to ensure their responsible integration into human-centric environments. Data and code are publicly available at \\url{https://github.com/agiresearch/TrustAgent}.","tags":[],"title":"TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution","type":"publication"},{"authors":["Wenyue Hua","Jiang Guo","Mingwen Dong","Henghui Zhu","Patrick Ng","Zhiguo Wang"],"categories":null,"content":" Abstract Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works; the featured strategies are not reusable and not adaptive to new tasks. This paper proposes a new pipeline to build OIE systems, where an Open-domain Information eXpression (OIX) task is proposed to provide a platform for all OIE strategies. The OIX is an OIE friendly expression of a sentence without information loss. The generation procedure of OIX contains shared works of OIE algorithms so that OIE strategies can be developed on the platform of OIX as inference operations focusing on more critical problems. Based on the same platform of OIX, the OIE strategies are reusable, and people can select a set of strategies to assemble their algorithm for a specific task so that the adaptability may be significantly increased. This paper focuses on the task of OIX and propose a solution–Open Information Annotation (OIA). OIA is a predicate-function-argument annotation for sentences. We label a data set of sentence-OIA pairs and propose a dependency-based rule system to generate OIA annotations from sentences. The evaluation results reveal that learning the OIA from a sentence is a challenge owing to the complexity of natural language sentences, and it is worthy of attracting more attention from the research community.\n","date":1703980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703980800,"objectID":"ece1c9bbaf9dae6db91ec13c7ff38f11","permalink":"https://Wenyueh.github.io/en/publication/recoe/","publishdate":"2023-12-31T00:00:00Z","relpermalink":"/en/publication/recoe/","section":"publication","summary":"Current approaches of knowledge editing struggle to effectively propagate updates to interconnected facts. In this work, we delve into the barriers that hinder the appropriate propagation of updated knowledge within these models for accurate reasoning. To support our analysis, we introduce a novel reasoning-based benchmark -- ReCoE (Reasoning-based Counterfactual Editing dataset) -- which covers six common reasoning schemes in real world. We conduct a thorough analysis of existing knowledge editing techniques, including input augmentation, finetuning, and locate-and-edit. We found that all model editing methods show notably low performance on this dataset, especially in certain reasoning schemes. Our analysis over the chain-of-thought generation of edited models further uncover key reasons behind the inadequacy of existing knowledge editing methods from a reasoning standpoint, involving aspects on fact-wise editing, fact recall ability, and coherence in generation. We will make our benchmark publicly available.","tags":[],"title":"Propagation and Pitfalls: Reasoning-based Assessment of Knowledge Editing through Counterfactual Tasks","type":"publication"},{"authors":["Wenyue Hua"],"categories":null,"content":" Abstract Large language models (LLMs) like OpenAI’s GPT series have revolutionized the field of natural language processing (NLP) by demonstrating an impressive ability to understand and generate human language. A key aspect of these models is their reasoning ability, which is a subject of growing interest and investigation. I am particularly focused on exploring the reasoning capabilities in LLMs. This includes understanding the mechanisms that facilitate reasoning within these models, assessing the extent to which LLMs are capable of conducting reasoning processes, and discerning between genuine reasoning and the mere mimicking of patterns observed in pre-trained data.\n","date":1703203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703203200,"objectID":"c39c7149c0b63887ff6f15c3e6386925","permalink":"https://Wenyueh.github.io/en/project/LLM-NLP/","publishdate":"2023-12-22T00:00:00Z","relpermalink":"/en/project/LLM-NLP/","section":"project","summary":"Large language models (LLMs) like OpenAI's GPT series have revolutionized the field of natural language processing (NLP) by demonstrating an impressive ability to understand and generate human language. A key aspect of these models is their reasoning ability, which is a subject of growing interest and investigation. I am particularly focused on exploring the reasoning capabilities in LLMs. This includes understanding the mechanisms that facilitate reasoning within these models, assessing the extent to which LLMs are capable of conducting reasoning processes, and discerning between genuine reasoning and the mere mimicking of patterns observed in pre-trained data.","tags":[],"title":"LLM \u0026 NLP","type":"project"},{"authors":["Lizhou Fan","Wenyue Hua","Lingyao Li","Haoyang Ling","Yongfeng Zhang","Libby Hemphill"],"categories":null,"content":" Abstract Complex reasoning ability is one of the most important features of current LLMs, which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of Large Language Models (LLMs) is critical – numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named NPHardEval. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class. These questions are meticulously chosen to represent a wide range of complexity class below the NP-hard complexity class, offering a rigorous measure of the reasoning ability of LLMs. Through this study, we shed light on the current state of reasoning in LLMs, providing an objective and rigorous perspective through the comparison of LLMs’ performance across complex classes. Moreover, this benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of NPHardEval are available at this https URL.\n","date":1703203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703203200,"objectID":"e29b6cc9bdd96d9faeebc8af5018669a","permalink":"https://Wenyueh.github.io/en/publication/nphard/","publishdate":"2023-12-22T00:00:00Z","relpermalink":"/en/publication/nphard/","section":"publication","summary":"Complex reasoning ability is one of the most important features of current LLMs, which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of Large Language Models (LLMs) is critical -- numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named NPHardEval. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class. These questions are meticulously chosen to represent a wide range of complexity class below the NP-hard complexity class, offering a rigorous measure of the reasoning ability of LLMs. Through this study, we shed light on the current state of reasoning in LLMs, providing an objective and rigorous perspective through the comparison of LLMs' performance across complex classes. Moreover, this benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of NPHardEval are available at [this https URL](https://github.com/casmlab/NPHardEval).","tags":[],"title":"NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes","type":"publication"},{"authors":["Yingqiang Ge","Yujie Ren","Wenyue Hua","Shuyuan Xu","Juntao Tan","Yongfeng Zhang"],"categories":null,"content":" Abstract This paper envisions a revolutionary AIOS-Agent ecosystem, where Large Language Model (LLM) serves as the (Artificial) Intelligent Operating System (IOS, or AIOS)–an operating system “with soul”. Upon this foundation, a diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift from the traditional OS-APP ecosystem. We envision that LLMs impact will not be limited to the AI application level, instead, it will in turn revolutionize the design and implementation of computer system, architecture, software, and programming language, featured by several main concepts. LLM as OS (system-level), Agents as Applications (application-level), Natural Language as Programming Interface (user-level), and Tools as Devices/Libraries (hardware/middleware-level). In this paper, we begin by introducing the architecture and historical evolution of traditional Operating Systems (OS). Then we formalize a conceptual framework for AIOS through “LLM as OS (LLMAO)”, drawing analogies between AIOS components and traditional OS elements. LLM is likened to OS kernel, context window to memory, external storage to file system, hardware tools to peripheral devices, software tools to programming libraries, and user prompts to user commands. Subsequently, we introduce the new AIOS-Agent Ecosystem, where users and developers can easily program Agent Applications (AAPs) using natural language, democratizing the development of and the access to computer software, which is different from the traditional OS-APP ecosystem, where desktop or mobile applications (APPs) have to be programmed by well-trained software developers using professional programming languages. Following this, we explore the diverse scope of Agent Applications. These agents can autonomously perform diverse tasks, showcasing intelligent task-solving ability in various scenarios. We delve into both single agent systems and multi-agent systems, as well as human-agent interaction. Lastly, we posit that the AIOS-Agent ecosystem can gain invaluable insights from the development trajectory of the traditional OS-APP ecosystem. Drawing on these insights, we propose a strategic roadmap for the evolution of the AIOS-Agent ecosystem. This roadmap is designed to guide the future research and development, suggesting systematic progresses of AIOS and its Agent applications.\n","date":1701907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701907200,"objectID":"ea3959db866a0568780b5a0bc2782a08","permalink":"https://Wenyueh.github.io/en/publication/LLMAO/","publishdate":"2023-12-07T00:00:00Z","relpermalink":"/en/publication/LLMAO/","section":"publication","summary":"This paper envisions a revolutionary AIOS-Agent ecosystem, where Large Language Model (LLM) serves as the (Artificial) Intelligent Operating System (IOS, or AIOS)--an operating system \"with soul\". Upon this foundation, a diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift from the traditional OS-APP ecosystem. We envision that LLMs impact will not be limited to the AI application level, instead, it will in turn revolutionize the design and implementation of computer system, architecture, software, and programming language, featured by several main concepts. LLM as OS (system-level), Agents as Applications (application-level), Natural Language as Programming Interface (user-level), and Tools as Devices/Libraries (hardware/middleware-level). In this paper, we begin by introducing the architecture and historical evolution of traditional Operating Systems (OS). Then we formalize a conceptual framework for AIOS through \"LLM as OS (LLMAO)\", drawing analogies between AIOS components and traditional OS elements. LLM is likened to OS kernel, context window to memory, external storage to file system, hardware tools to peripheral devices, software tools to programming libraries, and user prompts to user commands. Subsequently, we introduce the new AIOS-Agent Ecosystem, where users and developers can easily program Agent Applications (AAPs) using natural language, democratizing the development of and the access to computer software, which is different from the traditional OS-APP ecosystem, where desktop or mobile applications (APPs) have to be programmed by well-trained software developers using professional programming languages. Following this, we explore the diverse scope of Agent Applications. These agents can autonomously perform diverse tasks, showcasing intelligent task-solving ability in various scenarios. We delve into both single agent systems and multi-agent systems, as well as human-agent interaction. Lastly, we posit that the AIOS-Agent ecosystem can gain invaluable insights from the development trajectory of the traditional OS-APP ecosystem. Drawing on these insights, we propose a strategic roadmap for the evolution of the AIOS-Agent ecosystem. This roadmap is designed to guide the future research and development, suggesting systematic progresses of AIOS and its Agent applications.","tags":[],"title":"LLM as OS (llmao), Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem","type":"publication"},{"authors":["Wenyue Hua"],"categories":null,"content":" Abstract Artificial Intelligence (AI) aims to emulate Human Intelligence (HI) in combining basic skills to address complex tasks. AI agents is an especially important step the development of AI, which should integrate expert models and external tools for solving intricate problems, a step towards achieving Artificial General Intelligence (AGI). Large Language Models (LLMs) demonstrate notable capabilities in learning and reasoning and are proficient in employing external models, tools, plugins, or APIs for complex problem-solving. LLM-based agents are essentially LLMs enhanced with access to these additional resources.\n","date":1701129600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701129600,"objectID":"6c8318064bcaf848627ab86e36351b75","permalink":"https://Wenyueh.github.io/en/project/Agent/","publishdate":"2023-11-28T00:00:00Z","relpermalink":"/en/project/Agent/","section":"project","summary":"Artificial Intelligence (AI) aims to emulate Human Intelligence (HI) in combining basic skills to address complex tasks. AI agents is an especially important step the development of AI, which should integrate expert models and external tools for solving intricate problems, a step towards achieving Artificial General Intelligence (AGI). Large Language Models (LLMs) demonstrate notable capabilities in learning and reasoning and are proficient in employing external models, tools, plugins, or APIs for complex problem-solving. LLM-based agents are essentially LLMs enhanced with access to these additional resources.","tags":[],"title":"LLM-based Agent and Multi-agent System","type":"project"},{"authors":["Wenyue Hua","Lizhou Fan","Lingyao Li","Kai Mei","Jianchao Ji","Yingqiang Ge","Libby Hemphill","Yongfeng Zhang"],"categories":null,"content":" Abstract Can we avoid wars at the crossroads of history? This question has been pursued by individuals, scholars, policymakers, and organizations throughout human history. In this research, we attempt to answer the question based on the recent advances of Artificial Intelligence (AI) and Large Language Models (LLMs). We propose WarAgent, an LLM-powered multi-agent AI system, to simulate the participating countries, their decisions, and the consequences, in historical international conflicts, including the World War I (WWI), the World War II (WWII), and the Warring States Period (WSP) in Ancient China. By evaluating the simulation effectiveness, we examine the advancements and limitations of cutting-edge AI systems’ abilities in studying complex collective human behaviors such as international conflicts under diverse settings. In these simulations, the emergent interactions among agents also offer a novel perspective for examining the triggers and conditions that lead to war. Our findings offer data-driven and AI-augmented insights that can redefine how we approach conflict resolution and peacekeeping strategies. The implications stretch beyond historical analysis, offering a blueprint for using AI to understand human history and possibly prevent future international conflicts. Code and data are available at this url.\n","date":1701129600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701129600,"objectID":"6aec2c851a505250b9fc9f46304f927d","permalink":"https://Wenyueh.github.io/en/publication/WarAgent/","publishdate":"2023-11-28T00:00:00Z","relpermalink":"/en/publication/WarAgent/","section":"publication","summary":"Can we avoid wars at the crossroads of history? This question has been pursued by individuals, scholars, policymakers, and organizations throughout human history. In this research, we attempt to answer the question based on the recent advances of Artificial Intelligence (AI) and Large Language Models (LLMs). We propose **WarAgent**, an LLM-powered multi-agent AI system, to simulate the participating countries, their decisions, and the consequences, in historical international conflicts, including the World War I (WWI), the World War II (WWII), and the Warring States Period (WSP) in Ancient China. By evaluating the simulation effectiveness, we examine the advancements and limitations of cutting-edge AI systems' abilities in studying complex collective human behaviors such as international conflicts under diverse settings. In these simulations, the emergent interactions among agents also offer a novel perspective for examining the triggers and conditions that lead to war. Our findings offer data-driven and AI-augmented insights that can redefine how we approach conflict resolution and peacekeeping strategies. The implications stretch beyond historical analysis, offering a blueprint for using AI to understand human history and possibly prevent future international conflicts. Code and data are available at [this url](https://github.com/agiresearch/WarAgent).","tags":[],"title":"War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars","type":"publication"},{"authors":["Wenyue Hua","Lei Li","Shuyuan Xu","Chen Li","Yongfeng Zhang"],"categories":null,"content":" Abstract Foundation Models such as Large Language Models (LLMs) have significantly advanced many research areas. In particular, LLMs offer significant advantages for recommender systems, making them valuable tools for personalized recommendations. For example, by formulating various recommendation tasks such as rating prediction, sequential recommendation, straightforward recommendation, and explanation generation into language instructions, LLMs make it possible to build universal recommendation engines that can handle different recommendation tasks. Additionally, LLMs have a remarkable capacity for understanding natural language, enabling them to comprehend user preferences, item descriptions, and contextual information to generate more accurate and relevant recommendations, leading to improved user satisfaction and engagement. This tutorial introduces Foundation Models such as LLMs for recommendation. We will introduce how recommender system advanced from shallow models to deep models and to large models, how LLMs enable generative recommendation in contrast to traditional discriminative recommendation, and how to build LLM-based recommender systems. We will cover multiple perspectives of LLM-based recommendation, including data preparation, model design, model pre-training, fine-tuning and prompting, multi-modality and multi-task learning, as well as trustworthy perspectives of LLM-based recommender systems such as fairness and transparency.\n","date":1694390400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694390400,"objectID":"2e0e855900f68fd6db5662048739b9bf","permalink":"https://Wenyueh.github.io/en/publication/tutorial/","publishdate":"2023-09-11T00:00:00Z","relpermalink":"/en/publication/tutorial/","section":"publication","summary":"Foundation Models such as Large Language Models (LLMs) have significantly advanced many research areas. In particular, LLMs offer significant advantages for recommender systems, making them valuable tools for personalized recommendations. For example, by formulating various recommendation tasks such as rating prediction, sequential recommendation, straightforward recommendation, and explanation generation into language instructions, LLMs make it possible to build universal recommendation engines that can handle different recommendation tasks. Additionally, LLMs have a remarkable capacity for understanding natural language, enabling them to comprehend user preferences, item descriptions, and contextual information to generate more accurate and relevant recommendations, leading to improved user satisfaction and engagement. This tutorial introduces Foundation Models such as LLMs for recommendation. We will introduce how recommender system advanced from shallow models to deep models and to large models, how LLMs enable generative recommendation in contrast to traditional discriminative recommendation, and how to build LLM-based recommender systems. We will cover multiple perspectives of LLM-based recommendation, including data preparation, model design, model pre-training, fine-tuning and prompting, multi-modality and multi-task learning, as well as trustworthy perspectives of LLM-based recommender systems such as fairness and transparency.","tags":[],"title":"Tutorial on Large Language Models for Recommendation","type":"publication"},{"authors":["Shuyuan Xu","Wenyue Hua","Yongfeng Zhang"],"categories":null,"content":" Abstract This paper presents OpenP5, an open-source library for benchmarking foundation models for recommendation under the Pre-train, Personalized Prompt and Predict Paradigm (P5). We consider the implementation of P5 on three dimensions – 1) downstream task, 2) recommendation dataset, and 3) item indexing method. For 1), we provide implementation over two downstream tasks – sequential recommendation and straightforward recommendation. For 2), we surveyed frequently used datasets in recommender system research in recent years and provide implementation on ten datasets. In particular, we provide both single-dataset implementation and the corresponding checkpoints (P5) and another Super P5 (SP5) implementation that is pre-trained on all of the datasets, which supports recommendation across various domains with one model. For 3), we provide implementation of three item indexing methods to create item IDs – random indexing, sequential indexing, and collaborative indexing. We also provide comprehensive evaluation results of the library over the two downstream tasks, the ten datasets, and the three item indexing methods to facilitate reproducibility and future research. We open-source the code and the pre-trained checkpoints of the OpenP5 library at this url.\n","date":1687132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687132800,"objectID":"347e2cd11898875cd540ea074e1008e3","permalink":"https://Wenyueh.github.io/en/publication/openp5/","publishdate":"2023-06-19T00:00:00Z","relpermalink":"/en/publication/openp5/","section":"publication","summary":"This paper presents OpenP5, an open-source library for benchmarking foundation models for recommendation under the Pre-train, Personalized Prompt and Predict Paradigm (P5). We consider the implementation of P5 on three dimensions -- 1) downstream task, 2) recommendation dataset, and 3) item indexing method. For 1), we provide implementation over two downstream tasks -- sequential recommendation and straightforward recommendation. For 2), we surveyed frequently used datasets in recommender system research in recent years and provide implementation on ten datasets. In particular, we provide both single-dataset implementation and the corresponding checkpoints (P5) and another Super P5 (SP5) implementation that is pre-trained on all of the datasets, which supports recommendation across various domains with one model. For 3), we provide implementation of three item indexing methods to create item IDs -- random indexing, sequential indexing, and collaborative indexing. We also provide comprehensive evaluation results of the library over the two downstream tasks, the ten datasets, and the three item indexing methods to facilitate reproducibility and future research. We open-source the code and the pre-trained checkpoints of the OpenP5 library at [this url](https://github.com/agiresearch/OpenP5).","tags":[],"title":"OpenP5: Benchmarking Foundation Models for Recommendation","type":"publication"},{"authors":["Wenyue Hua","Shuyuan Xu","Yingqiang Ge","Yongfeng Zhang"],"categories":null,"content":" Abstract Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item as in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text and hallucinated recommendations when deciding which item(s) to recommend, creating LLM-compatible item IDs to uniquely identify each item is essential for recommendation foundation models. In this study, we systematically examine the item ID creation and indexing problem for recommendation foundation models, using P5 as an example of the backbone LLM. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as random indexing, title indexing, and independent indexing. We then propose four simple yet effective solutions, including sequential indexing, collaborative indexing, semantic (content-based) indexing, and hybrid indexing. Our study highlights the significant influence of item indexing methods on the performance of LLM-based recommendation, and our results on real-world datasets validate the effectiveness of our proposed solutions. The research also demonstrates how recent advances on language modeling and traditional IR principles such as indexing can help each other for better learning and inference. Source code and data are available at this url.\n","date":1683763200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683763200,"objectID":"08dd04d4221e0c6c57dbe81260cb5eb1","permalink":"https://Wenyueh.github.io/en/publication/indexing/","publishdate":"2023-05-11T00:00:00Z","relpermalink":"/en/publication/indexing/","section":"publication","summary":"Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item as in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text and hallucinated recommendations when deciding which item(s) to recommend, creating LLM-compatible item IDs to uniquely identify each item is essential for recommendation foundation models. In this study, we systematically examine the item ID creation and indexing problem for recommendation foundation models, using P5 as an example of the backbone LLM. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as random indexing, title indexing, and independent indexing. We then propose four simple yet effective solutions, including sequential indexing, collaborative indexing, semantic (content-based) indexing, and hybrid indexing. Our study highlights the significant influence of item indexing methods on the performance of LLM-based recommendation, and our results on real-world datasets validate the effectiveness of our proposed solutions. The research also demonstrates how recent advances on language modeling and traditional IR principles such as indexing can help each other for better learning and inference. Source code and data are available at [this url](https://github.com/Wenyueh/LLM-RecSys-ID).","tags":[],"title":"How to Index Item IDs for Recommendation Foundation Models","type":"publication"},{"authors":["Wenyue Hua"],"categories":null,"content":" Abstract The integration of foundation models like Large Language Models (LLMs) into recommender systems (RS) marks a significant advancement in the field. Adapting LLMs to recommender systems that manage billions of users and items presents a complex yet crucial challenge. This exploration delves into the benefits and potential issues of utilizing LLMs within recommender systems, contributing to advancements in this area.\n","date":1683676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683676800,"objectID":"5924cdc2bcf731527540c4e39d02d51f","permalink":"https://Wenyueh.github.io/en/project/LLM4RS/","publishdate":"2023-05-10T00:00:00Z","relpermalink":"/en/project/LLM4RS/","section":"project","summary":"The integration of foundation models like Large Language Models (LLMs) into recommender systems (RS) marks a significant advancement in the field. Adapting LLMs to recommender systems that manage billions of users and items presents a complex yet crucial challenge. This exploration delves into the benefits and potential issues of utilizing LLMs within recommender systems, contributing to advancements in this area.","tags":[],"title":"LLM for Recommender System","type":"project"},{"authors":["Yingqiang Ge","Wenyue Hua","Kai Mei","Jianchao Ji","Juntao Tan","Shuyuan Xu","Zelong Li","Yongfeng Zhang"],"categories":null,"content":" Abstract Human Intelligence (HI) excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive AI Agents, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools, plugins, or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research and development platform designed for solving multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools, plugins, or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM’s task-solving ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project’s code, dataset, benchmarks, evaluation methods, and the UI demo to foster community involvement in AGI advancement this url.\n","date":1681084800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681084800,"objectID":"09d1abff74ecea69b67c8937a72ec53d","permalink":"https://Wenyueh.github.io/en/publication/OpenAGI/","publishdate":"2023-04-10T00:00:00Z","relpermalink":"/en/publication/OpenAGI/","section":"publication","summary":"Human Intelligence (HI) excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive AI Agents, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools, plugins, or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research and development platform designed for solving multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools, plugins, or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's task-solving ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation methods, and the UI demo to foster community involvement in AGI advancement here [this url](https://github.com/agiresearch/OpenAGI).","tags":[],"title":"OpenAGI: When LLM Meets Domain Experts","type":"publication"},{"authors":["Wenyue Hua","Yingqiang Ge","Shuyuan Xu","Jianchao Ji","Zelong Li","Yongfeng Zhang"],"categories":null,"content":" Abstract Recent advancements in foundation models such as large language models (LLM) have propelled them to the forefront of recommender systems (RS). Moreover, fairness in RS is critical since many users apply it for decision-making and demand fulfillment. However, at present, there is a lack of understanding regarding the level of fairness exhibited by recommendation foundation models and the appropriate methods for equitably treating different groups of users in foundation models. In this paper, we focus on user-side unfairness problem and show through a thorough examination that there is unfairness involved in LLMs that lead to unfair recommendation results. To eliminate bias from LLM for fairness-aware recommendation, we introduce a novel Unbiased P5 (UP5) foundation model based on Counterfactually-Fair-Prompting (CFP) techniques. CFP includes two sub-modules: a personalized prefix prompt that enhances fairness with respect to individual sensitive attributes, and a Prompt Mixture that integrates multiple counterfactually-fair prompts for a set of sensitive attributes. Experiments are conducted on two real-world datasets, MovieLens-1M and Insurance, and results are compared with both matching-based and sequential-based fairness-aware recommendation models. The results show that UP5 achieves better recommendation performance and meanwhile exhibits a high level of fairness.\n","date":1674172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674172800,"objectID":"a1ad4bca22cec1ee4a1afe1918e40519","permalink":"https://Wenyueh.github.io/en/publication/fairness/","publishdate":"2023-01-20T00:00:00Z","relpermalink":"/en/publication/fairness/","section":"publication","summary":"Recent advancements in foundation models such as large language models (LLM) have propelled them to the forefront of recommender systems (RS). Moreover, fairness in RS is critical since many users apply it for decision-making and demand fulfillment. However, at present, there is a lack of understanding regarding the level of fairness exhibited by recommendation foundation models and the appropriate methods for equitably treating different groups of users in foundation models. In this paper, we focus on user-side unfairness problem and show through a thorough examination that there is unfairness involved in LLMs that lead to unfair recommendation results. To eliminate bias from LLM for fairness-aware recommendation, we introduce a novel Unbiased P5 (UP5) foundation model based on Counterfactually-Fair-Prompting (CFP) techniques. CFP includes two sub-modules -- a personalized prefix prompt that enhances fairness with respect to individual sensitive attributes, and a Prompt Mixture that integrates multiple counterfactually-fair prompts for a set of sensitive attributes. Experiments are conducted on two real-world datasets, MovieLens-1M and Insurance, and results are compared with both matching-based and sequential-based fairness-aware recommendation models. The results show that UP5 achieves better recommendation performance and meanwhile exhibits a high level of fairness.","tags":[],"title":"UP5: Unbiased Foundation Model for Fairness-aware Recommendation","type":"publication"},{"authors":["Wenyue Hua","Lifeng Jin","Lingfeng Song","Haitao Mi","Yongfeng Zhang","Dong Yu"],"categories":null,"content":" Abstract Current natural language processing (NLP) models such as BERT and RoBERTa have achieved high overall performance, but they often make systematic errors due to bias or certain difficult features to learn. Thus research on slice detection models (SDM) which automatically identifies underperforming groups of datapoints has gradually caught more attention, which aims at both understanding model behaviors and providing insights for future model training and designing. However, there is little systematic research on SDM and quantitative evaluation of its assessment for NLP models. Our paper fills this gap by proposing “Discover, Explanation, Improvement” framework that discovers coherent and underperforming groups of datapoints and unites datapoints of each slice under human-understandable concepts; it also provides comprehensive evaluation tasks and the corresponding quantitative metrics, which enable convenient comparison for future works. Results show that our framework can accurately select error-prone datapoints with informative semantic features that summarize error patterns, based on which it directly boosts model performance by an average of 2.85 points based on trained models without tuning any parameters across multiple datasets.\n","date":1672444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672444800,"objectID":"e89c80ce52e9a1867cf5944473e00a69","permalink":"https://Wenyueh.github.io/en/publication/domino/","publishdate":"2022-12-31T00:00:00Z","relpermalink":"/en/publication/domino/","section":"publication","summary":"Current natural language processing (NLP) models such as BERT and RoBERTa have achieved high overall performance, but they often make systematic errors due to bias or certain difficult features to learn. Thus research on slice detection models (SDM) which automatically identifies underperforming groups of datapoints has gradually caught more attention, which aims at both understanding model behaviors and providing insights for future model training and designing. However, there is little systematic research on SDM and quantitative evaluation of its assessment for NLP models. Our paper fills this gap by proposing \"Discover, Explanation, Improvement\" framework that discovers coherent and underperforming groups of datapoints and unites datapoints of each slice under human-understandable concepts; it also provides comprehensive evaluation tasks and the corresponding quantitative metrics, which enable convenient comparison for future works. Results show that our framework can accurately select error-prone datapoints with informative semantic features that summarize error patterns, based on which it directly boosts model performance by an average of 2.85 points based on trained models without tuning any parameters across multiple datasets.","tags":[],"title":"Discover, Explanation, Improvement: Automatic Slice Detection Framework for Natural Language Processing","type":"publication"},{"authors":["Wenyue Hua","Yongfeng Zhang"],"categories":null,"content":" Abstract Logical reasoning is a challenge for many current NLP neural network models since it requires more than the ability of learning informative representations from data. Inspired by the Dual Process Theory in cognitive science—which proposes that human cognition process involves two stages: an intuitive, unconscious and fast process relying on perception calledSystem 1, and a logical, conscious and slow process performing complex reasoning called System 2—we leverage neural logic reasoning (System 2) on top of the representation learning models (System 1), which conducts explicit neural-based differentiable logical reasoning on top of the representations learned by the base neural models. Based on experiments on the commonsense knowledge graph completion task, we show that the two-system architecture always improves from its System 1 model alone. Experiments also show that both the rule-driven logical regularizer and the data-driven value regularizer are important and the performance improvement is marginal without the two regularizers, which indicates that learning from both logical prior and training data is important for reasoning tasks.\n","date":1656374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656374400,"objectID":"bdf4e9bac9b71aba71123848fef4e817","permalink":"https://Wenyueh.github.io/en/publication/system12/","publishdate":"2022-06-28T00:00:00Z","relpermalink":"/en/publication/system12/","section":"publication","summary":"Logical reasoning is a challenge for many current NLP neural network models since it requires more than the ability of learning informative representations from data. Inspired by the Dual Process Theory in cognitive science—which proposes that human cognition process involves two stages -- an intuitive, unconscious and fast process relying on perception calledSystem 1, and a logical, conscious and slow process performing complex reasoning called System 2—we leverage neural logic reasoning (System 2) on top of the representation learning models (System 1), which conducts explicit neural-based differentiable logical reasoning on top of the representations learned by the base neural models. Based on experiments on the commonsense knowledge graph completion task, we show that the two-system architecture always improves from its System 1 model alone. Experiments also show that both the rule-driven logical regularizer and the data-driven value regularizer are important and the performance improvement is marginal without the two regularizers, which indicates that learning from both logical prior and training data is important for reasoning tasks.","tags":[],"title":"System 1+ System 2= Better World: Neural-Symbolic Chain of Logic Reasoning","type":"publication"},{"authors":["Wenzheng Zhang","Wenyue Hua","Karl Stratos"],"categories":null,"content":" Abstract A conventional approach to entity linking is to first find mentions in a given document and then infer their underlying entities in the knowledge base. A well-known limitation of this approach is that it requires finding mentions without knowing their entities, which is unnatural and difficult. We present a new model that does not suffer from this limitation called EntQA, which stands for Entity linking as Question Answering. EntQA first proposes candidate entities with a fast retrieval module, and then scrutinizes the document to find mentions of each candidate with a powerful reader module. Our approach combines progress in entity linking with that in open-domain question answering and capitalizes on pretrained models for dense entity retrieval and reading comprehension. Unlike in previous works, we do not rely on a mention-candidates dictionary or large-scale weak supervision. EntQA achieves strong results on the GERBIL benchmarking platform.\n","date":1632960000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632960000,"objectID":"c209a6ce61d116a64a42f116584ca669","permalink":"https://Wenyueh.github.io/en/publication/entqa/","publishdate":"2021-09-30T00:00:00Z","relpermalink":"/en/publication/entqa/","section":"publication","summary":"A conventional approach to entity linking is to first find mentions in a given document and then infer their underlying entities in the knowledge base. A well-known limitation of this approach is that it requires finding mentions without knowing their entities, which is unnatural and difficult. We present a new model that does not suffer from this limitation called EntQA, which stands for Entity linking as Question Answering. EntQA first proposes candidate entities with a fast retrieval module, and then scrutinizes the document to find mentions of each candidate with a powerful reader module. Our approach combines progress in entity linking with that in open-domain question answering and capitalizes on pretrained models for dense entity retrieval and reading comprehension. Unlike in previous works, we do not rely on a mention-candidates dictionary or large-scale weak supervision. EntQA achieves strong results on the GERBIL benchmarking platform.","tags":[],"title":"EntQA: Entity linking as question answering","type":"publication"},{"authors":["Wenyue Hua"],"categories":null,"content":" Abstract Formal linguistics is a branch of linguistics that focuses on the study of language using formal methods derived from mathematics and logic. It aims to understand the underlying structure of language by constructing precise, well-defined models of its syntax, semantics, and phonology. The key aspects of formal linguistics include (1) Syntax – This involves the study of the rules and principles that govern the structure of sentences. Formal syntactic theories explore how words combine to form grammatical sentences and the underlying rules that govern these combinations. (2) Semantics – This aspect deals with the meaning of words, phrases, and sentences. Formal semantics seeks to represent and analyze the ways in which linguistic expressions can convey different meanings in different contexts. (3) Phonology – This is the study of the sound systems of languages, including the rules for combining sounds into meaningful units or words. Formal phonology models the abstract sound structures of language and their functional roles.\n","date":1606521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606521600,"objectID":"4d8efa444705759e8daeceb6c1bc3c1e","permalink":"https://Wenyueh.github.io/en/project/linguistics/","publishdate":"2020-11-28T00:00:00Z","relpermalink":"/en/project/linguistics/","section":"project","summary":"Formal linguistics is a branch of linguistics that focuses on the study of language using formal methods derived from mathematics and logic. It aims to understand the underlying structure of language by constructing precise, well-defined models of its syntax, semantics, and phonology. The key aspects of formal linguistics include (1) Syntax -- This involves the study of the rules and principles that govern the structure of sentences. Formal syntactic theories explore how words combine to form grammatical sentences and the underlying rules that govern these combinations. (2) Semantics -- This aspect deals with the meaning of words, phrases, and sentences. Formal semantics seeks to represent and analyze the ways in which linguistic expressions can convey different meanings in different contexts. (3) Phonology -- This is the study of the sound systems of languages, including the rules for combining sounds into meaningful units or words. Formal phonology models the abstract sound structures of language and their functional roles.","tags":[],"title":"Formal Linguistics and Computational Linguistics","type":"project"},{"authors":["Wenyue Hua","Adam Jardine"],"categories":null,"content":" Abstract This paper studies the learning of two functions given positive samples of their composition, motivated by an empirical problem in natural language phonology. Empirically relevant conditions under which this is possible are identified and a provably correct algorithm is given that can semi-strongly identify the two functions in polynomial time and data. In order to clearly illustrate the learning problem and related concepts, we focus on a simple subset of input strictly 2-local functions. But we further argue that the general learning procedure we propose can be extended to more general classes of functions.\n","date":1605830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605830400,"objectID":"6b2ff79205557860b2968e13f0b7bef9","permalink":"https://Wenyueh.github.io/en/publication/icgi/","publishdate":"2020-11-20T00:00:00Z","relpermalink":"/en/publication/icgi/","section":"publication","summary":"This paper studies the learning of two functions given positive samples of their composition, motivated by an empirical problem in natural language phonology. Empirically relevant conditions under which this is possible are identified and a provably correct algorithm is given that can semi-strongly identify the two functions in polynomial time and data. In order to clearly illustrate the learning problem and related concepts, we focus on a simple subset of input strictly 2-local functions. But we further argue that the general learning procedure we propose can be extended to more general classes of functions.","tags":[],"title":"Learning input strictly local functions from their composition","type":"publication"},{"authors":["Wenyue Hua","Adam Jardine","Huteng Dai"],"categories":null,"content":" Abstract The simultaneous inference of underlying representations (URs) and a phonological grammar from alternating surface representations (SRs) in a morphological paradigm is a core problem in phonological learning that only recently has seen progress (Tesar, 2014; Cotterell et al., 2015; Rasin et al., 2018). This paper proposes a learning algorithm that infers URs and phonological processes from SRs based on the hypothesis that phonological generalizations belong to restrictive subregular regions in the Chomsky Hierarchy (Heinz, 2018). We give a procedure that, given sequences of morphemes paired with SRs, learns URs and a phonological grammar that is an input strictly local (ISL; Chandlee, 2014; Chandlee \u0026amp; Heinz, 2018) function. ISL functions are exactly those which make changes in the output with respect to the local information in the input. For now, the procedure is restricted to simplex ISL processes; that is, those exhibiting a single change. However, this illustrates that restrictive computational principles, combined with major principles in phonological analysis, allow for significant progress in understanding how phonological grammars and URs are learned. The paper is organized as follows. Section 2 briefly introduces the paradigm of the learning algorithm. Section 3 discusses the computational structure encoded in the learner. Section 4 is a detailed explanation of the algorithm with a simple example as illustration. Section 5 compares this algorithm with other algorithms and presents its advantages. The last section concludes the paper.\n","date":1605830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605830400,"objectID":"0f029bc173364a24f87fafa83cf9dfe3","permalink":"https://Wenyueh.github.io/en/publication/underlying/","publishdate":"2020-11-20T00:00:00Z","relpermalink":"/en/publication/underlying/","section":"publication","summary":"The simultaneous inference of underlying representations (URs) and a phonological grammar from alternating surface representations (SRs) in a morphological paradigm is a core problem in phonological learning that only recently has seen progress (Tesar, 2014; Cotterell et al., 2015; Rasin et al., 2018). This paper proposes a learning algorithm that infers URs and phonological processes from SRs based on the hypothesis that phonological generalizations belong to restrictive subregular regions in the Chomsky Hierarchy (Heinz, 2018). We give a procedure that, given sequences of morphemes paired with SRs, learns URs and a phonological grammar that is an input strictly local (ISL; Chandlee, 2014; Chandlee \u0026 Heinz, 2018) function. ISL functions are exactly those which make changes in the output with respect to the local information in the input. For now, the procedure is restricted to simplex ISL processes; that is, those exhibiting a single change. However, this illustrates that restrictive computational principles, combined with major principles in phonological analysis, allow for significant progress in understanding how phonological grammars and URs are learned. The paper is organized as follows. Section 2 briefly introduces the paradigm of the learning algorithm. Section 3 discusses the computational structure encoded in the learner. Section 4 is a detailed explanation of the algorithm with a simple example as illustration. Section 5 compares this algorithm with other algorithms and presents its advantages. The last section concludes the paper.","tags":[],"title":"Learning Underlying Representations and Input-Strictly-Local Functions","type":"publication"},{"authors":["Wenyue Hua"],"categories":null,"content":" Abstract In addition to the substance in phonology, a number of researchers have argued that computation also matters in phonology. Using the data in Yavapai (Yuman language), I show that other than an OT analysis focusing mainly on substance, a computational analysis is necessary for explaining the complex syllabification processes and the frequencies of optional surface representations due to different syllabifications. I will use computational complexity encoded in subregular hierarchy as the main technical tool in the computational analysis. Our main hypothesis is that when both SRs are well-formed based on the syllable phonotactics, the one less complex to generate is more frequently attested. The paper shows that the syllabification pattern in Yavapai necessarily requires a computational motivation, which in turn shows that computational property is a crucial factor in phonological transformations.\n","date":1574208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574208000,"objectID":"adb426931ef1a3a35d2da474f34a2810","permalink":"https://Wenyueh.github.io/en/publication/yavapai/","publishdate":"2019-11-20T00:00:00Z","relpermalink":"/en/publication/yavapai/","section":"publication","summary":"In addition to the substance in phonology, a number of researchers have argued that computation also matters in phonology. Using the data in Yavapai (Yuman language), I show that other than an OT analysis focusing mainly on substance, a computational analysis is necessary for explaining the complex syllabification processes and the frequencies of optional surface representations due to different syllabifications. I will use computational complexity encoded in subregular hierarchy as the main technical tool in the computational analysis. Our main hypothesis is that when both SRs are well-formed based on the syllable phonotactics, the one less complex to generate is more frequently attested. The paper shows that the syllabification pattern in Yavapai necessarily requires a computational motivation, which in turn shows that computational property is a crucial factor in phonological transformations.","tags":[],"title":"Computational Complexity in Optional Syllabification of Yavapai","type":"publication"},{"authors":["Wenyue Hua"],"categories":null,"content":" Abstract Disjunction is used to connect multiple options when there is insufficient information to determine which one is true. However, free choice disjunction defies this simple fact – when a disjunction acts as a free choice operator, all options are true. The usage of free choice disjunction is observed to usually co-occur with modals, pluralities, etc. This paper introduces a novel observation of sentences with free choice disjunction in English. Multiple data points are presented to demonstrate the semantic distribution. I propose that free choice reading in disjunctions depends on three factors –the sentence’s genericity, scope of the disjunction and disjuncts being imperfect nominals.\n","date":1561852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561852800,"objectID":"0027027aa0cc13c53bf3c9e1140c5771","permalink":"https://Wenyueh.github.io/en/publication/freechoice/","publishdate":"2019-06-30T00:00:00Z","relpermalink":"/en/publication/freechoice/","section":"publication","summary":"Disjunction is used to connect multiple options when there is insufficient information to determine which one is true. However, free choice disjunction defies this simple fact -- when a disjunction acts as a free choice operator, all options are true. The usage of free choice disjunction is observed to usually co-occur with modals, pluralities, etc. This paper introduces a novel observation of sentences with free choice disjunction in English. Multiple data points are presented to demonstrate the semantic distribution. I propose that free choice reading in disjunctions depends on three factors --the sentence’s genericity, scope of the disjunction and disjuncts being imperfect nominals.","tags":[],"title":"Free choice disjunction of propositions in generic sentences","type":"publication"},{"authors":["Mingming Sun","Wenyue Hua","Zoey Liu","Xin Wang","Kangjie Zheng","Ping Li"],"categories":null,"content":" Abstract Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works; the featured strategies are not reusable and not adaptive to new tasks. This paper proposes a new pipeline to build OIE systems, where an Open-domain Information eXpression (OIX) task is proposed to provide a platform for all OIE strategies. The OIX is an OIE friendly expression of a sentence without information loss. The generation procedure of OIX contains shared works of OIE algorithms so that OIE strategies can be developed on the platform of OIX as inference operations focusing on more critical problems. Based on the same platform of OIX, the OIE strategies are reusable, and people can select a set of strategies to assemble their algorithm for a specific task so that the adaptability may be significantly increased. This paper focuses on the task of OIX and propose a solution–Open Information Annotation (OIA). OIA is a predicate-function-argument annotation for sentences. We label a data set of sentence-OIA pairs and propose a dependency-based rule system to generate OIA annotations from sentences. The evaluation results reveal that learning the OIA from a sentence is a challenge owing to the complexity of natural language sentences, and it is worthy of attracting more attention from the research community.\n","date":1560988800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560988800,"objectID":"49ece731e6a8a530861894960f0e4ea7","permalink":"https://Wenyueh.github.io/en/publication/predicate/","publishdate":"2019-06-20T00:00:00Z","relpermalink":"/en/publication/predicate/","section":"publication","summary":"Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works; the featured strategies are not reusable and not adaptive to new tasks. This paper proposes a new pipeline to build OIE systems, where an Open-domain Information eXpression (OIX) task is proposed to provide a platform for all OIE strategies. The OIX is an OIE friendly expression of a sentence without information loss. The generation procedure of OIX contains shared works of OIE algorithms so that OIE strategies can be developed on the platform of OIX as inference operations focusing on more critical problems. Based on the same platform of OIX, the OIE strategies are reusable, and people can select a set of strategies to assemble their algorithm for a specific task so that the adaptability may be significantly increased. This paper focuses on the task of OIX and propose a solution–Open Information Annotation (OIA). OIA is a predicate-function-argument annotation for sentences. We label a data set of sentence-OIA pairs and propose a dependency-based rule system to generate OIA annotations from sentences. The evaluation results reveal that learning the OIA from a sentence is a challenge owing to the complexity of natural language sentences, and it is worthy of attracting more attention from the research community.","tags":[],"title":"A Predicate-Function-Argument Annotation of Natural Language for Open-Domain Information eXpression","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://Wenyueh.github.io/en/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/en/contact/","section":"","summary":"Hello!","tags":null,"title":"Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"f1d044c0738ab9f19347f15c290a71a1","permalink":"https://Wenyueh.github.io/en/research/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/en/research/","section":"","summary":"Hello!","tags":null,"title":"My Research","type":"widget_page"}]